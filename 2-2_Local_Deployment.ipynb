{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2.2: Deployment and Inference with MLflow\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zenml-io/zenbytes/blob/main/2-2_Local_Deployment.ipynb)\n",
    "\n",
    "***Key Concepts:*** *Model Deployer, MLflow*\n",
    "\n",
    "In the last lesson, we learned how to use MLflow and Weights & Biases to track our experiments and compare models. In the end, we found which hyperparameters produced the best-performing model on our validation dataset. How do we make this model available to our customers/users and enable them to query it?\n",
    "\n",
    "Setting up a dynamically scalable, highly-available, and reliable model service is a complex problem, and many companies hire large MLOps teams to build and maintain such services. With ZenML, we can build sophisticated ML services in a matter of minutes. In this lesson, we will start with a very basic model deployment, where we will use the [MLflow Models](https://mlflow.org/docs/latest/models.html) component to deploy our model as a local application that we can interact with via REST API. \n",
    "\n",
    "The beauty of ZenML is that our code can stay the same, no matter what tools or infrastructure we use. In a later chapter, we will see how this enables us to deploy the code we write here as a dynamically-scalable serverless microservice in the cloud. But more on that later.\n",
    "\n",
    "First, let's setup zenml and import some of the core steps we have created in previous lessons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zenml matplotlib\n",
    "!zenml integration install sklearn mlflow -y\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB ONLY setup\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "\n",
    "    # clone zenbytes repo to get source code of previous lessons\n",
    "    !git clone https://github.com/zenml-io/zenbytes.git  # noqa\n",
    "    !mv zenbytes/steps .\n",
    "    !mv zenbytes/pipelines .\n",
    "\n",
    "except ModuleNotFoundError as err:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.evaluator import evaluator\n",
    "from steps.importer import importer\n",
    "from steps.mlflow_trainer import svc_trainer_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML provides a standard step for deployment to MLflow, so we don't need to write any code ourselves. To deploy our model after training it, all we need to do is to add the `mlflow_model_deployer_step` into our pipeline. In addition to the trained model, this step expects a boolean argument of whether to deploy the model or not. This is very useful in practice, as it allows you to define some requirements for deploying your models, i.e., that it performs better than the currently deployed model or that no data drift is happening. For now, let us define a `deployment_trigger` that only deploys a model if the test accuracy is over 90%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.steps import step\n",
    "\n",
    "\n",
    "@step\n",
    "def deployment_trigger(test_acc: float) -> bool:\n",
    "    \"\"\"Only deploy if the test accuracy > 90%.\"\"\"\n",
    "    return test_acc > 0.9\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def train_evaluate_deploy_pipeline(\n",
    "    importer,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    deployment_trigger,\n",
    "    model_deployer,\n",
    "):\n",
    "    \"\"\"Train and deploy a model with MLflow.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    test_acc = evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    deployment_decision = deployment_trigger(test_acc)  # new\n",
    "    model_deployer(deployment_decision, model)  # new\n",
    "\n",
    "\n",
    "mlflow_pipeline = train_evaluate_deploy_pipeline(\n",
    "    importer=importer(),\n",
    "    trainer=svc_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    deployment_trigger=deployment_trigger(),  # new\n",
    "    model_deployer=mlflow_model_deployer_step(),  # new\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a new MLOps stack component, we need to register it with ZenML again before being able to run our pipeline. \n",
    "Similar to registering the experiment tracker in the last notebook, we first define a new model deployer, then add it to our ZenML stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change back to our default MLOps stack (in case W&B stack is still active)\n",
    "!zenml stack set default\n",
    "\n",
    "# Define MLflow experiment tracker from last lesson\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow --flavor=mlflow\n",
    "\n",
    "# Add the MLflow components into our default stack\n",
    "!zenml stack update default -d mlflow -e mlflow_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing `pipeline.run()` will now automatically deploy our model using MLflow. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the following command to get a list of all models currently deployed with our ZenML stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml served-models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a checkmark under status, the model was correctly deployed. Congrats!\n",
    "\n",
    "To interact with our deployed model in Python, we can use the `find_model_server()` method of ZenMLs model-deployer stack component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.repository import Repository\n",
    "\n",
    "repo = Repository()\n",
    "model_deployer = repo.active_stack.model_deployer\n",
    "services = model_deployer.find_model_server(\n",
    "    pipeline_name=\"train_evaluate_deploy_pipeline\",\n",
    "    pipeline_step_name=\"mlflow_model_deployer_step\",\n",
    "    running=True,\n",
    ")\n",
    "service = services[0]\n",
    "service.check_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with our model service a bit and send it a request. \n",
    "\n",
    "First, let's query the artifact store to get a sample from the test set of our last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = repo.get_pipeline(\"train_evaluate_deploy_pipeline\")\n",
    "last_run = p.runs[-1]\n",
    "X_test = last_run.steps[0].outputs[\"X_test\"].read()\n",
    "y_test = last_run.steps[0].outputs[\"y_test\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use matplotlib to plot the sample and see what our model would predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(X_test[0].reshape(8, 8), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "pred0 = service.predict(X_test[0:1])\n",
    "print(f\"Model predicted {pred0}, true label was {y_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we have deployed our first ML pipeline and learned how to interact with it. In practice, you would, of course, not query the model service manually but automatically send samples to it as new data comes in. That is what we will do in the [next lesson](2-3_Inference_Pipelines.ipynb), where we will build a basic inference pipeline. See you there!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a35bb4b4bceaf970a493ff7351e9d97180ab3fe9951c21e9e29c55a687242182"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('zenbytes-latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
