{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.1: Train-Test Skew Detection with Evidently\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zenml-io/zenbytes/blob/main/3-1_Data_Skew.ipynb)\n",
    "\n",
    "***Key Concepts:*** *Data-Centric ML, Data Skew, Train-Test Skew, Training-Serving Skew, Evidently*\n",
    "\n",
    "In academia and research, the focus of ML is usually to build the best possible models for a given dataset. However, in practical applications, the overall performance of our application is often determined primarily by data quality, not by the model. That is why many ML practitioners advocate for **Data-Centric** ML approaches, where we focus on improving the data while keeping the ML model (mostly) fixed. See [this great article](https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning) by neptune.ai for more details on model-centric vs. data-centric ML.\n",
    "\n",
    "One of the most critical parts of data-centric ML is to monitor data quality. Throughout this chapter, we will learn about many potential data issues, such as train-test skew, training-serving skew, data drift, and more. Being aware of these issues, and having respective safety mechanisms in place, is essential when serving ML models to real users.\n",
    "\n",
    "In this lesson, we will start by automatically checking for **Data Skew** within our ML pipelines. Since the performance of ML models on unseen data can be unpredictable, we should always try to design our training data to match the actual environment where our model will later be deployed. The difference between those data distributions is called **Training-Serving Skew**. Similarly, differences in distribution between our training and testing datasets are called **Train-Test Skew**.\n",
    "\n",
    "In the following, we will use the open-source data monitoring tool [Evidently](https://evidentlyai.com/) to measure distribution differences between our datasets. See this little [blog post](https://blog.zenml.io/zenml-loves-evidently/) of ours that explains the Evidently integration in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done so, install Evidently by running the following cell. This will also restart your notebook kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"zenml[server]\"\n",
    "!zenml integration install sklearn mlflow evidently -y\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "if Environment.in_google_colab():  # Colab only setup\n",
    "\n",
    "    # clone zenbytes repo to get source code of previous lessons\n",
    "    !git clone https://github.com/zenml-io/zenbytes.git  # noqa\n",
    "    !mv zenbytes/steps .\n",
    "    !mv zenbytes/pipelines ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Train-Test Skew\n",
    "\n",
    "First, we will use Evidently to check for skew between our training and test datasets. To do so, we will define a new pipeline with an Evidently step, into which we will then pass our training and test datasets. \n",
    "\n",
    "At its core, Evidently’s distribution difference calculation functions take in a reference dataset and compare it with a separate comparison dataset. These are both passed in as [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), though CSV inputs are also possible. ZenML implements this functionality in the form of several standardized steps along with an easy way to use the visualization tools also provided along with Evidently as ‘Dashboards’.\n",
    "\n",
    "Since our datasets were initially in [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) format, we also need to add another simple step that converts from numpy to pandas. The overall pipeline will then look like this:\n",
    "\n",
    "![Pipeline2](_assets/3-1/second_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this pipeline in code and import the other steps (which we have already built during previous lessons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.evaluator import evaluator\n",
    "from steps.importer import importer\n",
    "from steps.sklearn_trainer import svc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def digits_pipeline_with_train_test_checks(\n",
    "    importer,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    get_reference_data,\n",
    "    skew_detector,\n",
    "):\n",
    "    \"\"\"Digits pipeline with train-test check.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = importer()\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    reference, comparison = get_reference_data(X_train, X_test)\n",
    "    skew_detector(reference, comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the two new steps. For data distribution comparison, we can simply use the predefined step of ZenMLs Evidently integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileParameters,\n",
    "    evidently_profile_step,\n",
    ")\n",
    "\n",
    "# configure the Evidently step\n",
    "evidently_profile_params = EvidentlyProfileParameters(\n",
    "    profile_sections=[\"datadrift\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step for converting numpy to pandas is also fairly easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "\n",
    "@step\n",
    "def get_reference_data(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> Output(reference=pd.DataFrame, comparison=pd.DataFrame):\n",
    "    \"\"\"Convert numpy data to pandas for distribution difference calculation.\"\"\"\n",
    "    columns = [str(x) for x in list(range(X_train.shape[1]))]\n",
    "    X_train = pd.DataFrame(X_test, columns=columns)\n",
    "    X_test = pd.DataFrame(X_train, columns=columns)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before we can run the pipeline, we still need to add Evidently into our ZenML MLOps stack as a data validator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml data-validator register evidently_validator --flavor=evidently\n",
    "\n",
    "!zenml stack update default -dv evidently_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Let's initialize and run our pipeline to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_pipeline = digits_pipeline_with_train_test_checks(\n",
    "    importer=importer(),\n",
    "    trainer=svc_trainer(),\n",
    "    evaluator=evaluator(),\n",
    "    get_reference_data=get_reference_data(),\n",
    "    skew_detector=evidently_profile_step(\n",
    "        step_name=\"evidently_skew_detector\",\n",
    "        params=evidently_profile_params,\n",
    "    ),\n",
    ")\n",
    "evidently_pipeline.run(unlisted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use ZenMLs `EvidentlyVisualizer` to see the distribution comparison right in our notebook, where we can visually compare each feature's distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "from zenml.post_execution import get_unlisted_runs\n",
    "\n",
    "last_run = get_unlisted_runs()[-1]\n",
    "\n",
    "skew_detection_step = last_run.get_step(step=\"skew_detector\")\n",
    "evidently_outputs = skew_detection_step\n",
    "\n",
    "EvidentlyVisualizer().visualize(evidently_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is no skew between our training and test sets. That's great!\n",
    "\n",
    "In the following lessons, we will add training-serving skew and data drift detection mechanisms into our inference pipeline. We will also set up triggers for automated alerts whenever any data issues are detected. Those lessons are still work in progress, so stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('zenbytes-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec45946565c50b1d690aa5a9e3c974f5b62b9cc8d8934e441e52186140f79402"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
