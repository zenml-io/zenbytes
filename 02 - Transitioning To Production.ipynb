{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenML: Open-source MLOps Framework for reproducible ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](_assets/Logo/zenml.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> This lesson is still in progress and some commands may not work as described. Please expect an update until 20th April 2022. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging as absl_logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "absl_logging.set_verbosity(-10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by initializing ZenML in our directory. We are going to use a local stack to begin with, for simplicity and then transition to other stacks. This can be achieved in code by executing the following block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize ZenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf .zen\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML handles integrations natively, to avoid dependency conflicts, so make sure to use the following command to install the integrations required for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![All](_assets/integrations_all.png \"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![All](_assets/seldon.png \"Seldon Model Deployer.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install kubeflow seldon s3 aws -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Concept of MLOps Stacks\n",
    "\n",
    "The ZenML stack is a concept that describes the union of Metadata Store, Artifact Store and Orchestrator that will be used for all pipeline runs. When you get started with zenml you start off with a default local stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Local Stack\n",
    "\n",
    "You can imagine the local stack to look like this. Within the diagram we show how a generic pipeline interacts with the local stack.\n",
    "\n",
    "![LocalStack](_assets/localstack.png \"LocalStack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kubeflow Pipelines stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Kubeflow integration to extend the concept of stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to transition to a kubeflow stack that will look a little bit like this. Note that for kubeflow pipelines we also need a registry where the docker images for each step are registered. \n",
    "\n",
    "![KubeflowStack](_assets/aws_stack_seldon.png \"KubeflowStack\")\n",
    "\n",
    "But we have good news! You barely have to do anything to transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitioning to Production with Kubeflow on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps to follow in order to continue.\n",
    "\n",
    "- Set up the neccessary cloud resources on the provider of your choice\n",
    "- Configure ZenML with a new stack to be able to communicate with these resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up using the cloud guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to continue, it is best to follow the updated cloud guide for ZenML found [here](https://docs.zenml.io/features/guide-aws-gcp-azure). Please return after finishing the `pre-requisites` section.\n",
    "\n",
    "It is recommended you use AWS as your cloud provider to follow along the lesson. However, if you were to select GCP or Azure, it should not so hard to actually modify the below commands to work accordingly.\n",
    "\n",
    "You will also need Seldon Core to be installed in the same Kubernetes cluster as Kubeflow. Some brief instructions on how to install Seldon Core in AWS EKS can be found [here](https://github.com/zenml-io/zenml/tree/main/examples/seldon_deployment#installing-seldon-core-eg-in-an-eks-cluster). It is also advisable to read the official [Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your AWS Kubeflow Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can configure a new stack that points to your newly created resources on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember from the main README, Kubernetes and Docker are a pre-requisite to this part of the guide. Please make sure you have them installed. You also need to install the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the following with your own configuration. Use the below as exemplary.\n",
    "\n",
    "KUBE_CONTEXT=\"zenml-eks\"\n",
    "AWS_EKS_CLUSTER=\"zenhacks-cluster\"\n",
    "AWS_REGION=\"us-east-1\"\n",
    "ECR_REGISTRY_NAME=\"715803424590.dkr.ecr.us-east-1.amazonaws.com\"\n",
    "S3_BUCKET_NAME=\"s3://zenbytes-bucket\"\n",
    "KUBEFLOW_NAMESPACE=\"kubeflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up local access to the AWS EKS cluster and the AWS ECR registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Docker to the ECR registry\n",
    "!aws ecr get-login-password --region {AWS_REGION} | docker login --username AWS --password-stdin {ECR_REGISTRY_NAME}\n",
    "\n",
    "# Create a Kubernetes configuration context that points to the EKS cluster\n",
    "!aws eks --region {AWS_REGION} update-kubeconfig --name {AWS_EKS_CLUSTER} --alias {KUBE_CONTEXT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the base URL that will be used by Seldon Core to expose all model servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INGRESS_HOST = ! echo $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n",
    "INGRESS_HOST[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register the ZenML Stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Register container registry\n",
    "!zenml container-registry register ecr_registry --type=default --uri={ECR_REGISTRY_NAME}\n",
    "\n",
    "# Register orchestrator (Kubeflow on AWS)\n",
    "!zenml orchestrator register eks_orchestrator --type=kubeflow --kubernetes_context={KUBE_CONTEXT} --synchronous=True\n",
    "\n",
    "# Register metadata store and artifact store\n",
    "!zenml metadata-store register kubeflow_metadata_store --type=kubeflow\n",
    "!zenml artifact-store register s3_store --type=s3 --path={S3_BUCKET_NAME}\n",
    "\n",
    "# Register the Seldon Core model deployer (Seldon on AWS)\n",
    "!zenml model-deployer register eks_seldon --type=seldon --kubernetes_context={KUBE_CONTEXT} --kubernetes_namespace={KUBEFLOW_NAMESPACE} --base_url=http://{INGRESS_HOST[0]} --secret=s3_store\n",
    "\n",
    "# Register a secret manager\n",
    "!zenml secrets-manager register aws_secret_manager --type=aws\n",
    "\n",
    "# Register the aws_kubeflow_stack\n",
    "!zenml stack register aws_kubeflow_stack -m kubeflow_metadata_store -a s3_store -o eks_orchestrator -c ecr_registry -d eks_seldon -x aws_secret_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack set aws_kubeflow_stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set up a ZenML Secret to give Seldon Core access to the AWS S3 artifact store in the configured namespace, by running the `zenml secret register` command.\n",
    "\n",
    "NOTE: this is based on the assumption that Seldon Core is running in an EKS cluster that already has IAM access enabled and doesn't need any explicit AWS credentials. For more information on setting up ZenML secrets for Seldon Core, please see the [Managing Seldon Core Credentials](https://github.com/zenml-io/zenml/blob/main/examples/seldon_deployment/README.md#managing-seldon-core-credentials) section in our [Seldon Core Continuous Deployment Example](https://github.com/zenml-io/zenml/blob/main/examples/seldon_deployment/README.md).\n",
    "\n",
    "For the IAM access case, you can run this command to create the secret:\n",
    "\n",
    "`zenml secret register -s seldon_s3 s3_store`\n",
    "\n",
    ", and only set the `rclone_config_s3_env_auth` key to `True`. However, we cannot do this in the Jupyter Notebook, because interactive CLI commands are not supported, so we'll do it programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.repository import Repository\n",
    "from zenml.integrations.seldon.secret_schemas import SeldonS3SecretSchema\n",
    "\n",
    "secrets_manager = Repository().active_stack.secrets_manager\n",
    "secret = SeldonS3SecretSchema(\n",
    "    name = \"s3_store\",\n",
    "    rclone_config_s3_env_auth = True\n",
    ")\n",
    "try:\n",
    "    secrets_manager.get_secret(\"s3_store\")\n",
    "except RuntimeError:\n",
    "    secrets_manager.register_secret(secret)\n",
    "\n",
    "!zenml secret get s3_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will indicate the URL that we can access to view Kubeflow pipelines locally (e.g. [http://localhost:8080/](http://localhost:8080/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition to Production (Run on the Cloud)\n",
    "\n",
    "Once the stack is configured, all that is left to do is to set it active and to run a pipeline. Note that the code itself DOES NOT need to change, only the active stack.\n",
    "\n",
    "ZenML will detect that the stack has changed, and instead of running your pipeline locally, will build a Docker Image, push it to the container registry with your requirements, and deploy the pipeline with that image on Kubeflow Pipelines. This whole process is usually very painful but simplified with ZenML, and is completely customizable.\n",
    "\n",
    "For now, try it out! It might take a few minutes to build and push the image, but after that you'd see your pipeline in the cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Currently running pipelines defined within a jupyter notebook cell is\n",
    "    not supported. To get around this you can run the train pipeline within this repo. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train within kubeflow pipelines - this will deploy the pipeline\n",
    "!python run.py --deploy # --interval-second=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml served-models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats it you have successfully transitioned from local to production by simply switching you ZenML stack. This is just scratching the surface!\n",
    "\n",
    "Next up, more about stacks, running pipelines on a schedule, and much more coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Once you are done running pipelines with the AWS stack, you can run the following command to stop the Seldon Core model server and the local daemons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml served-models delete <UUID-of-model-server>\n",
    "!zenml stack down -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
